"""
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º deepseek-r1
"""
import os
import re
import json
import yaml
from pathlib import Path
from typing import List, Dict, Any
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
llm = OllamaLLM(
    model="deepseek-r1:8b",
    base_url="http://host.docker.internal:11434",
    temperature=0.1
)

# –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞
semantic_chunking_prompt = PromptTemplate(
    input_variables=["text", "chunk_size"],
    template="""
    –¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ –¥–µ–ª–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –†–∞–∑–¥–µ–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ (—á–∞–Ω–∫–∏).
    –ö–∞–∂–¥—ã–π —á–∞–Ω–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–ª–Ω—É—é –º—ã—Å–ª—å –∏–ª–∏ —Å–≤—è–∑–∞–Ω–Ω—É—é –≥—Ä—É–ø–ø—É –º—ã—Å–ª–µ–π, –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—â—É—é {chunk_size} —Ç–æ–∫–µ–Ω–æ–≤.
    –í–ê–ñ–ù–û: 
    - –°–æ—Ö—Ä–∞–Ω—è–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ª–æ–≥–∏—á–µ—Å–∫—É—é —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
    - –ù–µ —Ä–∞–∑–±–∏–≤–∞–π —Ç–µ–∫—Å—Ç –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    - –£—á–∏—Ç—ã–≤–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ä–∞–∑–¥–µ–ª—ã, –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã, —Å—Ç–∞—Ç—å–∏)
    - –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–π –Ω–∞ —á–∞–Ω–∫–∏ –≤ –º–µ—Å—Ç–∞—Ö –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—É–∑
    
    –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è:
    {text}
    
    –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞ —Å—Ç—Ä–æ–∫, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫:
    [ "—á–∞–Ω–∫1", "—á–∞–Ω–∫2", ... ]
    """
)

def semantic_chunk_text(text: str, chunk_size: int = 512) -> List[str]:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é deepseek-r1
    """
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—á–µ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if len(text) < chunk_size * 3:  # –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞
        return [text]
    
    # –í—ã–∑–æ–≤ LLM –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
    chain = semantic_chunking_prompt | llm
    response = chain.invoke({"text": text, "chunk_size": chunk_size})
    
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
        # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –∫–æ–¥–∞
        clean_response = re.sub(r'```json\s*|\s*```', '', response, flags=re.DOTALL)
        chunks = json.loads(clean_response.strip())
        
        if isinstance(chunks, list):
            return [chunk for chunk in chunks if chunk.strip()]
        else:
            # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –º–∞—Å—Å–∏–≤–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–¥–∏–Ω —á–∞–Ω–∫
            return [text]
    except json.JSONDecodeError:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ")
        return simple_chunk_text(text, chunk_size)

def simple_chunk_text(text: str, chunk_size: int = 512) -> List[str]:
    """
    –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤
    """
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0
    
    for word in words:
        current_chunk.append(word)
        current_length += len(word)
        
        if current_length > chunk_size:
            chunk_text = " ".join(current_chunk)
            chunks.append(chunk_text)
            current_chunk = []
            current_length = 0
    
    if current_chunk:
        chunk_text = " ".join(current_chunk)
        chunks.append(chunk_text)
    
    return chunks

def process_markdown_files(input_dir: Path, output_dir: Path, chunk_size: int = 512):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ markdown —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
    """
    output_dir.mkdir(exist_ok=True)
    
    md_files = list(input_dir.glob("*.md"))
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(md_files)} markdown —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    for md_file in md_files:
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {md_file.name}")
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        content = md_file.read_text(encoding="utf-8")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º YAML –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
        yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
        
        if yaml_match:
            yaml_header = yaml_match.group(1)
            text_content = yaml_match.group(2)
            metadata = yaml.safe_load(yaml_header)
        else:
            text_content = content
            metadata = {}
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
        chunks = semantic_chunk_text(text_content, chunk_size)
        
        print(f"  –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        for i, chunk in enumerate(chunks):
            chunk_filename = f"{md_file.stem}_chunk_{i:03d}.md"
            chunk_path = output_dir / chunk_filename
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∞
            chunk_metadata = metadata.copy()
            chunk_metadata["chunk_id"] = i
            chunk_metadata["original_file"] = md_file.name
            chunk_metadata["chunk_count"] = len(chunks)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º YAML –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —á–∞–Ω–∫–∞
            yaml_header = yaml.dump(chunk_metadata, default_flow_style=False, allow_unicode=True)
            chunk_content = f"---\n{yaml_header}---\n\n{chunk}"
            
            chunk_path.write_text(chunk_content, encoding="utf-8")
            print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–∞–Ω–∫: {chunk_filename}")

def main():
    input_dir = Path("/app/output")  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ markdown —Ñ–∞–π–ª–∞–º–∏
    output_dir = Path("/app/semantic_chunks")  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤
    chunk_size = 512  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞")
    print(f"  –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {input_dir}")
    print(f"  –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
    print(f"  –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size}")
    
    process_markdown_files(input_dir, output_dir, chunk_size)
    
    print("\n‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")

if __name__ == "__main__":
    main()