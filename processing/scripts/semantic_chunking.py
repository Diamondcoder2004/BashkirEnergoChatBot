"""
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HuggingFaceEmbeddings –∏ SemanticChunker
"""
import os
import re
import yaml
from pathlib import Path
from langchain_huggingface import HuggingFaceEmbeddings  # –ò–°–ü–†–ê–í–¨ –ò–ú–ü–û–†–¢!
from langchain_experimental.text_splitter import SemanticChunker
from typing import List

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞ (—Ä—É—Å—Å–∫–∏–π SBERT)
chunker_embeddings = HuggingFaceEmbeddings(
    model_name="ai-forever/sbert_ru_base",  
    encode_kwargs={"normalize_embeddings": True}
)
# –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
def semantic_chunk_text(text: str, chunk_size: int = 512) -> List[str]:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é SemanticChunker
    """
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä SemanticChunker —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
    chunker = SemanticChunker(chunker_embeddings, breakpoint_threshold_type="interquartile")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    chunks = chunker.split_text(text)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –æ–≤–µ—Ä–ª—ç–ø–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    processed_chunks = []
    for i, chunk in enumerate(chunks):
        # –û—á–∏—Å—Ç–∫–∞ —á–∞–Ω–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        clean_chunk = re.sub(r'\n{3,}', '\n\n', chunk.strip())
        clean_chunk = re.sub(r'^\s+|\s+$', '', clean_chunk, flags=re.MULTILINE)
        
        # –ï—Å–ª–∏ —á–∞–Ω–∫ –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (256 —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∏–ª–∏ —Å–ª–µ–¥—É—é—â–∏–º
        if len(clean_chunk) < 256:
            if i > 0 and len(processed_chunks) > 0:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —á–∞–Ω–∫–æ–º
                processed_chunks[-1] = processed_chunks[-1] + " " + clean_chunk
            elif i < len(chunks) - 1:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–∏–º —á–∞–Ω–∫–æ–º
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—É—â–∏–π –∏ –æ–±—ä–µ–¥–∏–Ω–∏–º —Å–æ —Å–ª–µ–¥—É—é—â–∏–º
        else:
            processed_chunks.append(clean_chunk)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–≤–µ—Ä–ª—ç–ø 20% –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
    if len(processed_chunks) > 1:
        overlap_chunks = []
        for i, chunk in enumerate(processed_chunks):
            if i > 0:
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º 20% –æ–≤–µ—Ä–ª—ç–ø –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
                overlap_size = int(len(chunk) * 0.2)
                if overlap_size > 0:
                    # –ë–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è –æ–≤–µ—Ä–ª—ç–ø–∞ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º
                    overlap_text = chunk[:overlap_size]
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–≤–µ—Ä–ª—ç–ø –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É —á–∞–Ω–∫—É
                    processed_chunks[i-1] = processed_chunks[i-1] + " ... " + overlap_text
            overlap_chunks.append(chunk)
        processed_chunks = overlap_chunks
    
    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –æ–≤–µ—Ä–ª—ç–ø–∞
    final_chunks = []
    for chunk in processed_chunks:
        # –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —á–∞–Ω–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        clean_chunk = re.sub(r'\n{3,}', '\n\n', chunk.strip())
        clean_chunk = re.sub(r'^\s+|\s+$', '', clean_chunk, flags=re.MULTILINE)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        if len(clean_chunk) >= 256:
            final_chunks.append(clean_chunk)
        else:
            # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π, –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º
            if final_chunks:
                final_chunks[-1] = final_chunks[-1] + "\n\n" + clean_chunk
            else:
                # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ –∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –º–∞–ª, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º
                final_chunks.append(clean_chunk)
    
    return final_chunks

def process_markdown_files(input_dir: Path, output_dir: Path, chunk_size: int = 512):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ markdown —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
    """
    output_dir.mkdir(exist_ok=True)
    
    md_files = list(input_dir.glob("*.md"))
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(md_files)} markdown —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    for md_file in md_files:
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {md_file.name}")
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        content = md_file.read_text(encoding="utf-8")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º YAML –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
        yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
        
        if yaml_match:
            yaml_header = yaml_match.group(1)
            text_content = yaml_match.group(2)
            metadata = yaml.safe_load(yaml_header)
        else:
            text_content = content
            metadata = {}
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ —á–∞–Ω–∫–∏–Ω–≥–æ–º (–∫–∞–∫ –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –∫–æ–¥–µ)
        clean_text = re.sub(r'## –°—Ç—Ä–∞–Ω–∏—Ü–∞ \d+', '', text_content)
        clean_text = re.sub(r'!\[.*?\]\(.*?\)', '', clean_text)
        clean_text = re.sub(r'\[[^\]]*\]\(.*?\)', '', clean_text)
        clean_text = re.sub(r'\n{3,}', '\n\n', clean_text.strip())
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
        chunks = semantic_chunk_text(clean_text, chunk_size)
        
        print(f"  –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        for i, chunk in enumerate(chunks):
            chunk_filename = f"{md_file.stem}_chunk_{i:03d}.md"
            chunk_path = output_dir / chunk_filename
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∞
            chunk_metadata = metadata.copy()
            chunk_metadata["chunk_id"] = i
            chunk_metadata["original_file"] = md_file.name
            chunk_metadata["chunk_count"] = len(chunks)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º YAML –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —á–∞–Ω–∫–∞
            yaml_header = yaml.dump(chunk_metadata, default_flow_style=False, allow_unicode=True)
            chunk_content = f"---\n{yaml_header}---\n\n{chunk}"
            
            chunk_path.write_text(chunk_content, encoding="utf-8")
            print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–∞–Ω–∫: {chunk_filename}")

def main():
    input_dir = Path("/app/data/output")     # –±—ã–ª–æ /app/output
    output_dir = Path("/app/data/semantic_chunks")  # –±—ã–ª–æ /app/semantic_chunks
    chunk_size = 512  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞")
    print(f"  –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {input_dir}")
    print(f"  –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
    print(f"  –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size}")
    
    process_markdown_files(input_dir, output_dir, chunk_size)
    
    print("\n‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")

if __name__ == "__main__":
    main()