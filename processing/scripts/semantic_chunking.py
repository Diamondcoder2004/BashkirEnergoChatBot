"""
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
"""
import os
import re
import yaml
from pathlib import Path
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_experimental.text_splitter import SemanticChunker
from typing import List
from dotenv import load_dotenv

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π
os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'
os.environ['HF_HOME'] = '/root/.cache/huggingface'

load_dotenv('/app/.env')

EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'sentence-transformers/all-MiniLM-L6-v2')

print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ —Å –º–æ–¥–µ–ª—å—é {EMBEDDING_MODEL}...")

# HuggingFace —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –∫—ç—à–µ–º
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    cache_folder="/root/.cache/huggingface",
    encode_kwargs={"normalize_embeddings": True},
    model_kwargs={"device": "cuda"}
)

# ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ...

def clean_text_content(text: str) -> str:
    """–¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞"""
    # –£–¥–∞–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü
    text = re.sub(r'## –°—Ç—Ä–∞–Ω–∏—Ü–∞ \d+', '', text)
    # –£–¥–∞–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç)
    text = re.sub(r'\[([^\]]*)\]\(.*?\)', r'\1', text)
    # –£–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ä–æ—á–µ 10 —Å–∏–º–≤–æ–ª–æ–≤
    lines = text.split('\n')
    clean_lines = []
    for line in lines:
        stripped = line.strip()
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        if len(stripped) >= 15 and not re.match(r'^[\.\d\s\-‚Äì‚Äî]*$', stripped):
            clean_lines.append(stripped)
    text = '\n'.join(clean_lines)
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def is_meaningful_chunk(text: str, min_length: int = 400) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —á–∞–Ω–∫ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º"""
    if len(text) < min_length:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–∏–º–≤–æ–ª–æ–≤
    text_ratio = len(re.sub(r'\s', '', text)) / len(text)
    if text_ratio < 0.6:  # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—Ä–æ–±–µ–ª–æ–≤/–ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤
    words = text.split()
    if len(words) < 50:  # –ú–∏–Ω–∏–º—É–º 50 —Å–ª–æ–≤
        return False
    
    return True

def semantic_chunk_text(text: str) -> List[str]:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –∂–µ—Å—Ç–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
    """
    try:
        chunker = SemanticChunker(
            embeddings, 
            breakpoint_threshold_type="interquartile"
        )
        raw_chunks = chunker.split_text(text)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞: {e}")
        return []
    
    meaningful_chunks = []
    
    for chunk in raw_chunks:
        clean_chunk = clean_text_content(chunk)
        
        # –ñ–µ—Å—Ç–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        if is_meaningful_chunk(clean_chunk):
            meaningful_chunks.append(clean_chunk)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏
    merged_chunks = []
    current_chunk = ""
    
    for chunk in meaningful_chunks:
        if not current_chunk:
            current_chunk = chunk
        elif len(current_chunk) + len(chunk) < 1500:  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            current_chunk += "\n\n" + chunk
        else:
            if is_meaningful_chunk(current_chunk):
                merged_chunks.append(current_chunk)
            current_chunk = chunk
    
    if current_chunk and is_meaningful_chunk(current_chunk):
        merged_chunks.append(current_chunk)
    
    return merged_chunks

def process_markdown_files(input_dir: Path, output_dir: Path):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç markdown —Ñ–∞–π–ª—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏"""
    output_dir.mkdir(exist_ok=True)
    
    # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —á–∞–Ω–∫–∏
    for old_file in output_dir.glob("*.md"):
        old_file.unlink()
    
    md_files = list(input_dir.glob("*.md"))
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(md_files)} markdown —Ñ–∞–π–ª–æ–≤")
    
    total_created = 0
    total_skipped = 0
    
    for md_file in md_files:
        print(f"\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {md_file.name}")
        
        try:
            content = md_file.read_text(encoding="utf-8")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if yaml_match:
                yaml_header = yaml_match.group(1)
                text_content = yaml_match.group(2)
                try:
                    metadata = yaml.safe_load(yaml_header)
                except yaml.YAMLError:
                    metadata = {"source": md_file.name}
            else:
                text_content = content
                metadata = {"source": md_file.name}
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            clean_text = clean_text_content(text_content)
            
            if len(clean_text) < 500:
                print(f"  ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(clean_text)} —Å–∏–º–≤–æ–ª–æ–≤), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                total_skipped += 1
                continue
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
            chunks = semantic_chunk_text(clean_text)
            
            if not chunks:
                print(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏")
                total_skipped += 1
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏
            saved_count = 0
            for i, chunk in enumerate(chunks):
                chunk_filename = f"{md_file.stem}_chunk_{i:03d}.md"
                chunk_path = output_dir / chunk_filename
                
                chunk_metadata = metadata.copy()
                chunk_metadata.update({
                    "chunk_id": i,
                    "original_file": md_file.name,
                    "chunk_count": len(chunks),
                    "chunk_size": len(chunk),
                    "word_count": len(chunk.split())
                })
                
                yaml_header = yaml.dump(chunk_metadata, default_flow_style=False, allow_unicode=True)
                chunk_content = f"---\n{yaml_header}---\n\n{chunk}"
                
                chunk_path.write_text(chunk_content, encoding="utf-8")
                saved_count += 1
            
            print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            total_created += saved_count
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}")
            total_skipped += 1
    
    print(f"\nüìä –ò–¢–û–ì–ò:")
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {total_created}")
    print(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_skipped}")

def main():
    input_dir = Path("/app/data/output")
    output_dir = Path("/app/data/semantic_chunks")
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞")
    print(f"  üìÅ –í—Ö–æ–¥: {input_dir}")
    print(f"  üìÅ –í—ã—Ö–æ–¥: {output_dir}")
    
    if not input_dir.exists():
        print(f"‚ùå –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {input_dir}")
        return
    
    process_markdown_files(input_dir, output_dir)
    print("\n‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")

if __name__ == "__main__":
    main()