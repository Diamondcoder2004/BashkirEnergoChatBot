"""
–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤
"""
import os
import re
import yaml
from pathlib import Path
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_core.documents import Document
from dotenv import load_dotenv

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π
os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'
os.environ['HF_HOME'] = '/root/.cache/huggingface'

load_dotenv('/app/.env')

# –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ .env
COLLECTION_NAME = os.getenv('COLLECTION_NAME', 'bashkir_energo_docs')
QDRANT_HOST = os.getenv('QDRANT_HOST', 'localhost')
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'sentence-transformers/all-MiniLM-L6-v2')

print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã —Å –º–æ–¥–µ–ª—å—é {EMBEDDING_MODEL}...")

# HuggingFace —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –∫—ç—à–µ–º
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    cache_folder="/root/.cache/huggingface",
    encode_kwargs={"normalize_embeddings": True},
    model_kwargs={"device": "cuda"}
)

# ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ...

def filter_quality_chunks(documents: list) -> list:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É"""
    quality_docs = []
    skipped_count = 0
    
    for doc in documents:
        text = doc.page_content
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–∞
        if (len(text) >= 400 and 
            len(text.split()) >= 60 and  # –ú–∏–Ω–∏–º—É–º 60 —Å–ª–æ–≤
            not re.search(r'^\s*\d+\s*$', text) and  # –ù–µ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
            not re.search(r'^[\.\-\s]*$', text)):  # –ù–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–∫–∏ –∏ —Ç–∏—Ä–µ
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            clean_text = re.sub(r'\n{3,}', '\n\n', text.strip())
            doc.page_content = clean_text
            quality_docs.append(doc)
        else:
            skipped_count += 1
    
    print(f"üìä –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {len(quality_docs)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤, {skipped_count} –æ—Ç–±—Ä–æ—à–µ–Ω–æ")
    return quality_docs

def load_semantic_chunks() -> list:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏"""
    chunks_dir = Path("/app/data/semantic_chunks")
    
    if not chunks_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {chunks_dir}")
        return []
    
    md_files = list(chunks_dir.glob("*.md"))
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(md_files)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤")
    
    documents = []
    
    for md_file in md_files:
        try:
            content = md_file.read_text(encoding="utf-8")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç
            yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if yaml_match:
                yaml_header = yaml_match.group(1)
                text_content = yaml_match.group(2)
                
                try:
                    metadata = yaml.safe_load(yaml_header)
                except yaml.YAMLError:
                    metadata = {}
                
                # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
                clean_text = re.sub(r'\n{3,}', '\n\n', text_content.strip())
                
                documents.append(Document(
                    page_content=clean_text,
                    metadata={
                        **metadata,
                        "source_file": md_file.name,
                        "chunk_size": len(clean_text)
                    }
                ))
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–∞ {md_file.name}: {e}")
    
    return documents

def create_vector_store():
    """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ Qdrant"""
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤...")
    documents = load_semantic_chunks()
    
    if not documents:
        print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
    quality_documents = filter_quality_chunks(documents)
    
    if not quality_documents:
        print("‚ùå –ù–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        return
    
    print(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∏–∑ {len(quality_documents)} —á–∞–Ω–∫–æ–≤...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        vector_store = Qdrant.from_documents(
            documents=quality_documents,
            embedding=embeddings,
            url=f"http://{QDRANT_HOST}:6333",
            collection_name=COLLECTION_NAME,
            force_recreate=True
        )
        
        print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"   üìä –ö–æ–ª–ª–µ–∫—Ü–∏—è: {COLLECTION_NAME}")
        print(f"   üìà –í–µ–∫—Ç–æ—Ä–æ–≤: {len(quality_documents)}")
        print(f"   üîó Qdrant: http://{QDRANT_HOST}:6333/dashboard")
        
        return vector_store
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
        return None

def main():
    print(f"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print(f"   –ú–æ–¥–µ–ª—å: sentence-transformers/all-MiniLM-L6-v2")
    print(f"   –ö–æ–ª–ª–µ–∫—Ü–∏—è: {COLLECTION_NAME}")
    print(f"   Qdrant: {QDRANT_HOST}:6333")
    
    vector_store = create_vector_store()
    
    if vector_store:
        print("\nüéâ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    else:
        print("\nüí• –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É")

if __name__ == "__main__":
    main()