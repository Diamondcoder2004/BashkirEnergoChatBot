# main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
import uvicorn
import logging
from typing import List, Optional
import os
import sys
import time
import gc
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '..', '.env'))

from reranker import get_relevant_docs, get_relevant_chunks

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

COLLECTION_NAME = os.getenv("COLLECTION_NAME", "bashkir_energo_minilm_v2")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "bambucha/saiga-llama3")

app = FastAPI(
    title="RAG API –ë–∞—à–∫–∏—Ä—ç–Ω–µ—Ä–≥–æ",
    description="API –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –ë–∞—à–∫–∏—Ä—ç–Ω–µ—Ä–≥–æ",
    version="1.0.0"
)

class QuestionRequest(BaseModel):
    question: str
    top_k: Optional[int] = 3
    temperature: Optional[float] = 0.1
    rerank_threshold: Optional[float] = 0.1

class DocumentResponse(BaseModel):
    content: str
    source: str
    relevance_score: Optional[float] = None

class AnswerResponse(BaseModel):
    question: str
    answer: str
    sources: List[DocumentResponse]
    parameters: dict

class HealthResponse(BaseModel):
    status: str
    service: str
    collection: str

class ClearCacheResponse(BaseModel):
    status: str
    message: str
    gpu_cleared: bool

llm = None
vector_store = None
SERVICE_READY = False

def clear_gpu_memory():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ"""
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            logger.info("üßπ GPU memory cleared")
            return True
    except ImportError:
        logger.warning("‚ö†Ô∏è torch not available for GPU memory clearing")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è GPU memory clearing failed: {e}")
    
    # –í—Å–µ–≥–¥–∞ –¥–µ–ª–∞–µ–º garbage collection
    gc.collect()
    return False

@app.post("/clear-cache")
async def clear_cache():
    """
    –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU –∏ –ø–∞–º—è—Ç–∏
    """
    try:
        gpu_cleared = clear_gpu_memory()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è LangChain –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        try:
            from langchain.globals import set_llm_cache
            set_llm_cache(None)
            logger.info("üßπ LangChain cache cleared")
        except:
            pass
            
        return ClearCacheResponse(
            status="success",
            message="Cache cleared successfully",
            gpu_cleared=gpu_cleared
        )
    except Exception as e:
        logger.error(f"‚ùå Cache clearing error: {e}")
        return ClearCacheResponse(
            status="error",
            message=f"Cache clearing failed: {str(e)}",
            gpu_cleared=False
        )

def compress(text: str) -> str:
    try:
        return llm.invoke(f"–°–æ–∂–º–∏ —Ç–µ–∫—Å—Ç –¥–æ 20% –æ–±—ä—ë–º–∞, —Å–æ—Ö—Ä–∞–Ω–∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã:\n\n{text}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∂–∞—Ç–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        return text

@app.on_event("startup")
async def startup_event():
    global llm, vector_store, SERVICE_READY
    try:
        OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://host.docker.internal:11434")
        QDRANT_HOST = os.getenv("QDRANT_HOST", "host.docker.internal")
        
        logger.info(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama: {OLLAMA_HOST}")
        logger.info(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant: {QDRANT_HOST}")
        logger.info(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {COLLECTION_NAME}")
        logger.info(f"ü§ñ –ú–æ–¥–µ–ª—å LLM: {OLLAMA_MODEL}")
        
        time.sleep(5)
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                llm = OllamaLLM(
                    model=OLLAMA_MODEL,
                    base_url=OLLAMA_HOST,
                    temperature=0.1,
                    num_ctx=8192,
                    timeout=120
                )
                test_response = llm.invoke("–û—Ç–≤–µ—Ç—å 'OK'")
                logger.info(f"‚úÖ –¢–µ—Å—Ç Ollama: {test_response.strip()}")
                break
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    raise e
        
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                cache_folder="/root/.cache/huggingface",
                encode_kwargs={"normalize_embeddings": True},
                model_kwargs={"device": "cuda"}
            )
            logger.info("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            SERVICE_READY = False
            return
        
        try:
            qdrant_client = QdrantClient(host=QDRANT_HOST, port=6333, timeout=60)
            
            collections = qdrant_client.get_collections()
            collection_names = [col.name for col in collections.collections]
            logger.info(f"üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_names}")
            
            if COLLECTION_NAME not in collection_names:
                logger.warning(f"‚ùå –ö–æ–ª–ª–µ–∫—Ü–∏—è '{COLLECTION_NAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                SERVICE_READY = False
                return
            
            vector_store = Qdrant(
                client=qdrant_client,
                collection_name=COLLECTION_NAME,
                embeddings=embeddings
            )
            
            test_results = vector_store.similarity_search("–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ", k=1)
            logger.info(f"‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(test_results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Qdrant: {e}")
            SERVICE_READY = False
            return
        
        logger.info("‚úÖ RAG —Å–µ—Ä–≤–∏—Å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        SERVICE_READY = True
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
        SERVICE_READY = False

@app.get("/")
async def root():
    return {
        "message": "RAG API –ë–∞—à–∫–∏—Ä—ç–Ω–µ—Ä–≥–æ", 
        "status": "running",
        "collection": COLLECTION_NAME
    }

@app.get("/health")
async def health():
    return HealthResponse(
        status="ready" if SERVICE_READY else "degraded", 
        service="rag-api",
        collection=COLLECTION_NAME
    )

@app.get("/test")
async def test():
    return {
        "test": "ok", 
        "ready": SERVICE_READY,
        "collection": COLLECTION_NAME
    }

@app.post("/reset")
async def reset_llm():
    global llm
    try:
        OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://host.docker.internal:11434")
        llm = OllamaLLM(
            model=OLLAMA_MODEL,
            base_url=OLLAMA_HOST,
            temperature=0.1,
            num_ctx=8192,
            timeout=120
        )
        return {"status": "LLM reset successful"}
    except Exception as e:
        return {"status": f"Error: {str(e)}"}

@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    if not SERVICE_READY:
        return AnswerResponse(
            question=request.question,
            answer=f"–°–µ—Ä–≤–∏—Å –Ω–µ –≥–æ—Ç–æ–≤. –ö–æ–ª–ª–µ–∫—Ü–∏—è '{COLLECTION_NAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ —Å–µ—Ä–≤–∏—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.",
            sources=[],
            parameters={}
        )
    
    try:
        logger.info(f"üì• –í–æ–ø—Ä–æ—Å: {request.question}")
        logger.info(f"üéõÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: top_k={request.top_k}, temperature={request.temperature}, rerank_threshold={request.rerank_threshold}")
        
        global llm
        OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://host.docker.internal:11434")
        llm = OllamaLLM(
            model=OLLAMA_MODEL,
            base_url=OLLAMA_HOST,
            temperature=request.temperature,
            num_ctx=8192,
            timeout=120
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º top_k –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –Ω–æ —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 3 –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
        initial_search_k = max(request.top_k * 3, 10)  # –∏—â–µ–º –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
        initial_docs = get_relevant_chunks(request.question, vector_store, k=initial_search_k)
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(initial_docs)} (k={initial_search_k})")
        
        if not initial_docs:
            return AnswerResponse(
                question=request.question,
                answer="–í –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É",
                sources=[],
                parameters={
                    "top_k": request.top_k,
                    "temperature": request.temperature,
                    "rerank_threshold": request.rerank_threshold,
                    "initial_search_k": initial_search_k
                }
            )
        
        relevant_docs_with_scores = get_relevant_docs(request.question, initial_docs, request.rerank_threshold)
        logger.info(f"‚úÖ –û—Ç—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(relevant_docs_with_scores)}")
        
        for i, doc in enumerate(relevant_docs_with_scores[:5]):  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-5
            logger.info(f"üìä –î–æ–∫—É–º–µ–Ω—Ç {i+1}: score={doc['relevance_score']:.3f}")
        
        relevant_docs_with_scores = relevant_docs_with_scores[:request.top_k]
        
        if not relevant_docs_with_scores:
            return AnswerResponse(
                question=request.question,
                answer="–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –¥–∞–Ω–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É",
                sources=[],
                parameters={
                    "top_k": request.top_k,
                    "temperature": request.temperature,
                    "rerank_threshold": request.rerank_threshold,
                    "initial_search_k": initial_search_k
                }
            )
        
        context = "\n\n".join([doc['content'] for doc in relevant_docs_with_scores])
        logger.info(f"üìÑ –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω ({len(context)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        compressed_context = compress(context)
        logger.info(f"üì¶ –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–∂–∞—Ç –¥–æ ({len(compressed_context)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        prompt = f"""–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –ë–∞—à–∫–∏—Ä—ç–Ω–µ—Ä–≥–æ. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–ö–û–ù–¢–ï–ö–°–¢:
{compressed_context}

–í–û–ü–†–û–°:
{request.question}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç - —Å–∫–∞–∂–∏ "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É"
3. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º
4. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

–û–¢–í–ï–¢:"""
        
        logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
        answer = llm.invoke(prompt)
        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
        
        sources = []
        for i, doc in enumerate(relevant_docs_with_scores):
            sources.append(DocumentResponse(
                content=doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content'],
                source=f"–î–æ–∫—É–º–µ–Ω—Ç {i+1}",
                relevance_score=round(doc['relevance_score'], 3)
            ))
        
        return AnswerResponse(
            question=request.question,
            answer=answer.strip(),
            sources=sources,
            parameters={
                "top_k": request.top_k,
                "temperature": request.temperature,
                "rerank_threshold": request.rerank_threshold,
                "initial_search_k": initial_search_k
            }
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
        return AnswerResponse(
            question=request.question,
            answer=f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
            sources=[],
            parameters={
                "top_k": request.top_k,
                "temperature": request.temperature,
                "rerank_threshold": request.rerank_threshold,
                "initial_search_k": initial_search_k
            }
        )

if __name__ == "__main__":
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    )